{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d4a02f",
   "metadata": {},
   "source": [
    "# Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e7077",
   "metadata": {},
   "source": [
    "Foundation models benefit from huge training data.  The models and number of parameters are enormous.\n",
    "\n",
    "- ChatGPT used 45 TB of texts and 175 Billion paramters (1.287 GWh) for training.\n",
    "- DALL-E2 used 100's of Millions of images for training.\n",
    "\n",
    "The amount of power and energy that the data center uses to power ChatGPT is more than Orange County, Florida (this is where Disney World is located).\n",
    "\n",
    "<small>`Tons of data -> Training -> Foundation model -> Application (answer questions, sentiment analysis, search, etc.)`</small>\n",
    "\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "In 2017, Google introduced the concept of Transformers.  Some important concepts in this scope are:\n",
    "\n",
    "1. Parallelization\n",
    "1. Memory efficiency\n",
    "1. Adaptive focus/weighting\n",
    "1. Scalable to input/output length\n",
    "\n",
    "Transformers are the result of a progression in sequencing models.\n",
    "\n",
    "Believe it or not, transformers got their name from the popular cartoon/comic Transformers \"... because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\" (see sources).\n",
    "\n",
    "#### Progression of Sequence Models\n",
    "\n",
    "<small>`Recurrent Neural Network (RNN) -> Long Short-Term Memory (LSTM) -> Transformer`</small>\n",
    "\n",
    "**Recurrent Neural Network (RNN):** Sequence model that processes one step at a time, maintaining a memory of previous steps via a hidden state.  Struggles remembered information far back in the sequence.\n",
    "\n",
    "**Long Short-Term Memory (LSTM):** Handles long sequences due to vanishing gradients.  Forget gate, input gate, output gate.  Much better at long-term memory. Hangles vanishing gradients well.\n",
    "\n",
    "**Transformer:** The superhero squad of sequence models. It ditches recurrence entirely and uses attention mechanisms to handle long-range dependencies. This architecture powers GPT, BERT, and pretty much every modern language model.\n",
    "\n",
    "#### Concept of Attention\n",
    "\n",
    "- [Attention is All you Need (article)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "- [Attention Is All You Need (Wikipedia entry)](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03cec73",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n",
    "1. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97265a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
