{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df0cb7d-2948-4d65-b6e6-a8d3ecf99cb4",
   "metadata": {},
   "source": [
    "# Generative Artificial Intelligence (GenAI)\n",
    "### Tokenization\n",
    "\n",
    "Believe it or not, transformers got their name from the popular cartoon/comic Transformers \"... because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\" (see sources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d9d90",
   "metadata": {},
   "source": [
    "#### Progression of Sequence Models\n",
    "`Recurrent Neural Network (RNN) -> Long Short-Term Memory (LSTM) -> Transformer`\n",
    "\n",
    "*Recurrent Neural Network (RNN):* Sequence model that processes one step at a time, maintaining a memory of previous steps via a hidden state.  Struggles remembered information far back in the sequence.\n",
    "\n",
    "*Long Short-Term Memory (LSTM):* handles long sequences due to vanishing gradients.  Forget gate, input gate, output gate.  Much better at long-term memory. Hangles vanishing gradients well.\n",
    "\n",
    "*Transformer:* The superhero squad of sequence models. It ditches recurrence entirely and uses attention mechanisms to handle long-range dependencies. This architecture powers GPT, BERT, and pretty much every modern language model.\n",
    "\n",
    "This file shows some examples of Generative AI tokenization outputs.  The tokens are used within the model's context window (how much can it remember at once) in order to produce positional encodings that are used within the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a47efa",
   "metadata": {},
   "source": [
    "#### Tokenizing with OpenAI\n",
    "This snippet will use OpenAI's API to return the tokens for a given string.  Notice that the tokens may not include full words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e6ca4-c780-4043-9637-60fb29d656e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"Hello Moose, you code fast!\")\n",
    "ids = tokenizer.encode(\"Hello Moose, you code fast!\")\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf56dcf-af41-421a-8294-2e8e24830730",
   "metadata": {},
   "source": [
    "#### OpenAI transformers library uses underlying BPE (Byte Pair Encoding) called `tiktoken`.\n",
    "\n",
    "To install on your machine:\n",
    "\n",
    "- `pip install tiktoken` or\n",
    "- `pip3 install tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b24c2-0427-4a30-a7f1-33843ee15fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Get the encoding for a specific model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Encode a string into tokens\n",
    "tokens = encoding.encode(\"Hello, world!\")\n",
    "\n",
    "# Decode tokens back into a string\n",
    "text = encoding.decode(tokens)\n",
    "\n",
    "print(tokens)  # Outputs: [15496, 11, 995]\n",
    "print(text)    # Outputs: Hello, world!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d93c7",
   "metadata": {},
   "source": [
    "#### View token IDs that are embedded within the model's core training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e225de9-a04a-4815-b807-e564e9b5a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"Moose codes fast\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812bd50",
   "metadata": {},
   "source": [
    "#### Special Characters and Emojis\n",
    "\n",
    "The following snippet shows examples of how the tokenizer handles special characters and punctuation.  The tokenizer will place all punctuation, regardless of significance, into their own tokens.  \n",
    "\n",
    "Emojis are also tokenized into their own tokens.  The tokenizer will also split words into subwords if they are not in the vocabulary.  \n",
    "\n",
    "For example, \"Moose\" is not in the vocabulary, so it is split into \"Mo\" and \"ose\".  The tokenizer will also add a special token at the beginning and end of the text to indicate the start and end of the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778fab4-0dba-4305-8312-faca0813edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "samples = [\n",
    "    \"Moose codes fast!\",\n",
    "    \"unbelievable\",\n",
    "    \"üî•üêçüöÄ\",\n",
    "    \"def add(a, b): return a + b\",\n",
    "    \"What???!!\",\n",
    "    \"Caf√© r√©sum√©\"\n",
    "]\n",
    "\n",
    "for text in samples:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ids = tokenizer.encode(text)\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e048ea8",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n",
    "2. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
